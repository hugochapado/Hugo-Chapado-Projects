{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# Loading Data\n",
    "labels_names =['Stadium','Building','Traffic Sign','Forest','Flowers',\n",
    "              'Street','Classroom','Bridge','Statue','Lake']\n",
    "data_train = np.load('data_train.npy')\n",
    "labels_train = np.load('labels_train.npy')\n",
    "labels_train = labels_train.astype(int)\n",
    "class_labels, _ = np.unique(labels_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import errno\n",
    "#Converting data to directory\n",
    "transposed_data = data_train.T\n",
    "\n",
    "for idx, l in np.ndenumerate(labels_train):\n",
    "   # data = data_train[:,idx]\n",
    "    data = transposed_data[idx]\n",
    "    img = Image.fromarray( data.reshape((300,300,3)), 'RGB')\n",
    "    horz_img = img.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "    labelNr = l - 1\n",
    "    filename = f'KerasPrep/{labelNr}/{idx[0]}.jpg'\n",
    "    filename2 = f'KerasPrep/{labelNr}/{idx[0]}flip.jpg'\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "    img.save(filename)\n",
    "    horz_img.save(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "def build_retrained_model(batch_size, img_height, img_width, data_dir, validation_split,random_rotation,random_zoom,dropout,base_learning_rate):\n",
    "    # dividing in classes train and validation\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    num_classes = len(train_ds.class_names)\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "    \n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "     #flip the images and rotate and zoom to increase generalization.\n",
    "    data_augmentation = keras.Sequential(\n",
    "      [\n",
    "        layers.RandomFlip(\"horizontal\",\n",
    "                          input_shape=(img_height,\n",
    "                                      img_width,\n",
    "                                      3)),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomRotation(random_rotation),\n",
    "        layers.RandomZoom(random_zoom),\n",
    "        layers.Rescaling(1./255),\n",
    "      ]\n",
    "    )\n",
    "    # build model with ResNet50\n",
    "    model = Sequential([\n",
    "        data_augmentation,\n",
    "      ResNet50(include_top=False, weights='imagenet', pooling='max'),\n",
    "        layers.Flatten(),\n",
    "      layers.Dense(1028, activation='relu'),\n",
    "      layers.Dropout(dropout),\n",
    "      layers.Dense(num_classes)\n",
    "    ])\n",
    "    \n",
    "    return model, train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6248 files belonging to 10 classes.\n",
      "Using 4999 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 22:25:56.887842: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 22:25:57.949499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79124 MB memory:  -> device: 0, name: A100-SXM-80GB, pci bus id: 0000:87:00.0, compute capability: 8.0\n",
      "2021-12-10 22:25:57.952928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 79124 MB memory:  -> device: 1, name: A100-SXM-80GB, pci bus id: 0000:90:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6248 files belonging to 10 classes.\n",
      "Using 1249 files for validation.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1028)              2106372   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1028)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10290     \n",
      "=================================================================\n",
      "Total params: 25,704,374\n",
      "Trainable params: 25,651,254\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 22:26:04.566916: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-10 22:26:15.692380: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2021-12-10 22:26:17.101911: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Running ptxas --version returned 32512\n",
      "2021-12-10 22:26:17.235837: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 32512, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2021-12-10 22:26:19.817173: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 40s 112ms/step - loss: 2.5459 - accuracy: 0.6663 - val_loss: 9.7003 - val_accuracy: 0.1065\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.7239 - accuracy: 0.8382 - val_loss: 4.9958 - val_accuracy: 0.1249\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.4342 - accuracy: 0.8894 - val_loss: 3.9529 - val_accuracy: 0.2362\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.3644 - accuracy: 0.9086 - val_loss: 11.2240 - val_accuracy: 0.3347\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.2851 - accuracy: 0.9294 - val_loss: 2.7237 - val_accuracy: 0.6533\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 11s 71ms/step - loss: 0.2286 - accuracy: 0.9348 - val_loss: 0.7405 - val_accuracy: 0.8567\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.2279 - accuracy: 0.9456 - val_loss: 0.2769 - val_accuracy: 0.9343\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.2138 - accuracy: 0.9486 - val_loss: 0.2431 - val_accuracy: 0.9560\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 11s 71ms/step - loss: 0.1888 - accuracy: 0.9578 - val_loss: 2.1158 - val_accuracy: 0.8415\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.1677 - accuracy: 0.9588 - val_loss: 0.4162 - val_accuracy: 0.9311\n",
      "Epoch 10/40\n",
      "157/157 [==============================] - 18s 77ms/step - loss: 0.0780 - accuracy: 0.9764 - val_loss: 0.1110 - val_accuracy: 0.9720\n",
      "Epoch 11/40\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.0563 - accuracy: 0.9826 - val_loss: 0.1051 - val_accuracy: 0.9760\n",
      "Epoch 12/40\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.0384 - accuracy: 0.9872 - val_loss: 0.0948 - val_accuracy: 0.9784\n",
      "Epoch 13/40\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.0258 - accuracy: 0.9932 - val_loss: 0.0882 - val_accuracy: 0.9776\n",
      "Epoch 14/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.0906 - val_accuracy: 0.9784\n",
      "Epoch 15/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0261 - accuracy: 0.9918 - val_loss: 0.1003 - val_accuracy: 0.9760\n",
      "Epoch 16/40\n",
      "157/157 [==============================] - 11s 71ms/step - loss: 0.0262 - accuracy: 0.9910 - val_loss: 0.0928 - val_accuracy: 0.9800\n",
      "Epoch 17/40\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.0168 - accuracy: 0.9938 - val_loss: 0.0894 - val_accuracy: 0.9792\n",
      "Epoch 18/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0191 - accuracy: 0.9940 - val_loss: 0.0812 - val_accuracy: 0.9824\n",
      "Epoch 19/40\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.0151 - accuracy: 0.9944 - val_loss: 0.0831 - val_accuracy: 0.9848\n",
      "Epoch 20/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0150 - accuracy: 0.9948 - val_loss: 0.0887 - val_accuracy: 0.9864\n",
      "Epoch 21/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0134 - accuracy: 0.9942 - val_loss: 0.0831 - val_accuracy: 0.9848\n",
      "Epoch 22/40\n",
      "157/157 [==============================] - 11s 69ms/step - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.0848 - val_accuracy: 0.9848\n",
      "Epoch 23/40\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.0160 - accuracy: 0.9962 - val_loss: 0.0818 - val_accuracy: 0.9840\n",
      "Epoch 24/40\n",
      "157/157 [==============================] - 11s 71ms/step - loss: 0.0162 - accuracy: 0.9950 - val_loss: 0.0804 - val_accuracy: 0.9848\n",
      "Epoch 25/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.0819 - val_accuracy: 0.9856\n",
      "Epoch 26/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0137 - accuracy: 0.9968 - val_loss: 0.0785 - val_accuracy: 0.9888\n",
      "Epoch 27/40\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.0892 - val_accuracy: 0.9864\n",
      "Epoch 28/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0087 - accuracy: 0.9968 - val_loss: 0.0816 - val_accuracy: 0.9856\n",
      "Epoch 29/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0902 - val_accuracy: 0.9872\n",
      "Epoch 30/40\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.0105 - accuracy: 0.9968 - val_loss: 0.0857 - val_accuracy: 0.9872\n",
      "Epoch 31/40\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.0931 - val_accuracy: 0.9872\n",
      "Epoch 32/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0072 - accuracy: 0.9972 - val_loss: 0.0972 - val_accuracy: 0.9872\n",
      "Epoch 33/40\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.0118 - accuracy: 0.9968 - val_loss: 0.0968 - val_accuracy: 0.9880\n",
      "Epoch 34/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.0862 - val_accuracy: 0.9872\n",
      "Epoch 35/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0084 - accuracy: 0.9980 - val_loss: 0.0845 - val_accuracy: 0.9888\n",
      "Epoch 36/40\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.0072 - accuracy: 0.9964 - val_loss: 0.0807 - val_accuracy: 0.9888\n",
      "Epoch 37/40\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.0061 - accuracy: 0.9978 - val_loss: 0.0948 - val_accuracy: 0.9880\n",
      "Epoch 38/40\n",
      "157/157 [==============================] - 11s 70ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0999 - val_accuracy: 0.9872\n",
      "Epoch 39/40\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 0.0068 - accuracy: 0.9972 - val_loss: 0.1122 - val_accuracy: 0.9848\n",
      "Epoch 40/40\n",
      "157/157 [==============================] - 11s 68ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.1013 - val_accuracy: 0.9872\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the Model\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "data_dir = 'KerasPrep'\n",
    "validation_split = 0.2\n",
    "random_rotation = 0.35\n",
    "random_zoom = 0.35\n",
    "dropout = 0.24\n",
    "epochs=10\n",
    "base_learning_rate = 0.0001\n",
    "IMG_SHAPE = (224, 224) + (3,)\n",
    "\n",
    "\n",
    "# build base model\n",
    "retrained_model, train_ds, val_ds = build_retrained_model(batch_size, img_height, img_width, data_dir, \n",
    "                                             validation_split,random_rotation,random_zoom,\n",
    "                                             dropout,base_learning_rate)\n",
    "# use adam model optimizer.\n",
    "retrained_model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "retrained_model.summary()\n",
    "#train model\n",
    "history = retrained_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "    \n",
    "retrained_model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "total_epochs =  epochs + 30\n",
    "\n",
    "history_fine = retrained_model.fit(train_ds,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "acc_fine = history_fine.history['accuracy']\n",
    "val_acc_fine = history_fine.history['val_accuracy']\n",
    "\n",
    "loss_fine = history_fine.history['loss']\n",
    "val_loss_fine = history_fine.history['val_loss']\n",
    "\n",
    "epochs_range = range(total_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc + acc_fine, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc + val_acc_fine, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss + loss_fine, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss + val_loss_fine, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to 5 with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "# Test classification by downloading an image from the internet and then prediciting the class label (int value)\n",
    "\n",
    "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
    "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    sunflower_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = retrained_model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_labels[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_model(batch_size, img_height, img_width, data_dir, validation_split,random_rotation,random_zoom,dropout,dropout2,number_of_layers,batch_normalization,start_neurons):\n",
    "    # dividing in classes train and validation\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    num_classes = len(train_ds.class_names)\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "      data_dir,\n",
    "      validation_split=validation_split,\n",
    "      subset=\"validation\",\n",
    "      seed=123,\n",
    "      image_size=(img_height, img_width),\n",
    "      batch_size=batch_size)\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    #flip the images and rotate and zoom to increase generalization.\n",
    "    data_augmentation = keras.Sequential(\n",
    "      [\n",
    "        layers.RandomFlip(\"horizontal\",\n",
    "                          input_shape=(img_height,\n",
    "                                      img_width,\n",
    "                                      3)),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomRotation(random_rotation),\n",
    "        layers.RandomZoom(random_zoom),\n",
    "        layers.Rescaling(1./255),\n",
    "      ]\n",
    "    )\n",
    "    # Building the sequential model with relu acitivation function\n",
    "    \n",
    "    main_model = []\n",
    "    \n",
    "    for layer in range((number_of_layers)):\n",
    "        main_model += [layers.Conv2D((start_neurons * (2**layer)), 3, padding='same', activation='relu'),\n",
    "                       layers.MaxPooling2D(2),\n",
    "                       layers.Dropout(dropout)]\n",
    "        \n",
    "        if batch_normalization:\n",
    "            main_model.append(layers.BatchNormalization())\n",
    "    \n",
    "    model = Sequential([\n",
    "      data_augmentation,\n",
    "        \n",
    "        keras.Sequential(main_model),\n",
    "        \n",
    "      layers.Flatten(),\n",
    "      layers.Dense((start_neurons * (2**(number_of_layers-1))), activation='relu'),\n",
    "      layers.Dropout(dropout2),\n",
    "      layers.Dense(num_classes)\n",
    "    ])\n",
    "    \n",
    "    # use adam model optimizer.\n",
    "   \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \"\"\"\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=[keras.metrics.TopKCategoricalAccuracy(k=3, name='acc_top3'),\n",
    "                       keras.metrics.TopKCategoricalAccuracy(k=1, name='acc_top1')])\n",
    "    \"\"\"\n",
    "\n",
    "    model.summary()\n",
    "    return model, train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the Model\n",
    "batch_size = 32\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "data_dir = 'KerasPrep'\n",
    "validation_split = 0.2\n",
    "random_rotation = 0.35\n",
    "random_zoom = 0.35\n",
    "dropout = 0.3\n",
    "dropout2 = 0.31\n",
    "start_neurons = 128\n",
    "number_of_layers = 2\n",
    "batch_normalization = False\n",
    "epochs=230\n",
    "\n",
    "# build model\n",
    "custom_model, train_ds, val_ds = build_custom_model(batch_size, img_height, img_width, data_dir, \n",
    "                                             validation_split,random_rotation,random_zoom,\n",
    "                                             dropout,dropout2,number_of_layers,batch_normalization,\n",
    "                                             start_neurons)\n",
    "\n",
    "#train model\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', min_delta=0.001, patience=6, verbose=0, mode='max',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "history = custom_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  #callbacks = [callback]\n",
    ")\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification by downloading an image from the internet and then prediciting the class label (int value)\n",
    "\n",
    "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
    "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    sunflower_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = custom_model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_labels[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edits by C.Silva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9360613810741688\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Parameters\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Load data\n",
    "data_test = np.load('data_test.npy') \n",
    "labels_test = np.load('labels_test.npy')\n",
    "\n",
    "# Load model\n",
    "custom_model = models.load_model('retrained_model.h5')\n",
    "\n",
    "# Manually reshaping data into necessary format..\n",
    "X = data_test.reshape((300,300,3,data_test.shape[1]))\n",
    "pred_labels=[]\n",
    "for i in range(data_test.shape[1]):\n",
    "    # There is likely a better solution then using a for loop\n",
    "    # But this is the fastest implementation I found that worked\n",
    "    img = tf.image.resize(X[:,:,:,i], size=[img_height,img_width])\n",
    "    img1 =tf.expand_dims(img,0)\n",
    "    \n",
    "    # Predictions\n",
    "    predictions = custom_model.predict(img1)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    pred_labels += [class_labels[np.argmax(score)]]\n",
    "    \n",
    "pred_labels = np.array(pred_labels)\n",
    "print('Accuracy: ', accuracy_score(labels_test, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8f7ade48c80230ac5b84b17687fc16066a630bd01b7221445104590a037b0bd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
